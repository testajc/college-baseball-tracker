name: Daily Stats Scrape

on:
  schedule:
    # Run at 6 AM UTC every day (1 AM EST / 10 PM PST)
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      command:
        description: 'Command to run (run, recover, diagnostic, status)'
        required: false
        default: 'run'
      force:
        description: 'Force scrape even if season has not started (true/false)'
        required: false
        default: 'false'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hour timeout

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r scraper/requirements.txt

      - name: Check if season has started
        id: season_check
        run: |
          python -c "
          from datetime import datetime
          start = datetime(2026, 2, 14)
          if datetime.now() < start:
              print('Season has not started yet (starts Feb 14, 2026)')
              print('skip=true')
          else:
              print('Season is active')
              print('skip=false')
          " | tee /tmp/season_check.txt
          if grep -q 'skip=true' /tmp/season_check.txt; then
            echo "skip=true" >> $GITHUB_OUTPUT
          else
            echo "skip=false" >> $GITHUB_OUTPUT
          fi

      - name: Build schools database
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        run: |
          cd scraper
          python build_schools_db.py

      - name: Download previous progress
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        uses: actions/download-artifact@v4
        with:
          name: scrape-progress
          path: scraper/
        continue-on-error: true

      - name: Seed scrape history from DB if artifact missing
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          cd scraper
          if [ ! -f scrape_history.json ]; then
            echo "scrape_history.json missing (artifact expired?) — seeding from DB"
            python -c "
          import json
          from datetime import date
          from database import DatabaseManager
          db = DatabaseManager()
          teams = db.get_schools_in_db()
          db.close()
          if teams:
              today = date.today().isoformat()
              history = {
                  'initial_scrape_complete': True,
                  'last_scraped': {name: today for name in teams}
              }
              with open('scrape_history.json', 'w') as f:
                  json.dump(history, f, indent=2)
              print(f'Seeded scrape_history.json with {len(teams)} teams (last_scraped={today})')
          else:
              print('DB has no teams — starting fresh initial scrape')
          "
          else
            echo "scrape_history.json exists ($(wc -l < scrape_history.json) lines)"
          fi

      - name: Clean up bad data
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          cd scraper
          python main.py cleanup

      - name: Debug DB checkpoint
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          cd scraper
          python -c "
          from database import DatabaseManager
          from scheduler import SmartScheduler
          db = DatabaseManager()
          s = SmartScheduler()
          in_db = db.get_schools_in_db()
          schools = s.get_schools_to_scrape_today()
          remaining = [x for x in schools if x['school_name'] not in in_db]
          d1 = [x for x in remaining if x.get('division') == 'D1']
          d2 = [x for x in remaining if x.get('division') == 'D2']
          d3 = [x for x in remaining if x.get('division') == 'D3']
          report = []
          report.append(f'Teams in DB: {len(in_db)}')
          report.append(f'Sample DB teams: {sorted(list(in_db))[:10]}')
          report.append(f'Scheduler returned: {len(schools)}')
          report.append(f'After DB checkpoint: {len(remaining)} (D1={len(d1)} D2={len(d2)} D3={len(d3)})')
          report.append(f'First 20 to scrape: {[x[\"school_name\"]+\" (\"+x.get(\"division\",\"?\")+\")\" for x in remaining[:20]]}')
          text = '\n'.join(report)
          print(text)
          open('/tmp/debug_checkpoint.txt', 'w').write(text)
          db.close()
          "

      - name: Upload debug info
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: debug-checkpoint
          path: /tmp/debug_checkpoint.txt
          retention-days: 1

      - name: Run scraper
        if: steps.season_check.outputs.skip != 'true' || github.event.inputs.force == 'true'
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          cd scraper
          CMD="${{ github.event.inputs.command || 'run' }}"
          ARGS=""

          if [ "${{ github.event.inputs.force }}" = "true" ]; then
            ARGS="$ARGS --force"
          fi

          if [ "$CMD" = "diagnostic" ]; then
            python main.py diagnostic
          elif [ "$CMD" = "status" ]; then
            python main.py status
          elif [ "$CMD" = "recover" ]; then
            python main.py recover $ARGS
          else
            python main.py run $ARGS
          fi

      - name: Upload progress
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scrape-progress
          path: |
            scraper/scrape_history.json
            scraper/scraper.log
          retention-days: 90
          overwrite: true

      - name: Season not started notification
        if: steps.season_check.outputs.skip == 'true' && github.event.inputs.force != 'true'
        run: |
          echo "::notice::2026 college baseball season hasn't started yet (starts Feb 14, 2026). Skipping scrape."
          echo "To force a scrape anyway, use workflow_dispatch with force=true."
